{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Include.ipynb\n",
    "\n",
    "class Net_block(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def Conv2d(params):\n",
    "        # params: input_channels, out_channels, kernel_size,\n",
    "        # stride, padding, bias\n",
    "        return nn.Conv2d(params[0], params[1], params[2],\n",
    "                         params[3], params[4], bias=params[5])\n",
    "    \n",
    "    @staticmethod\n",
    "    def ConvT2d(params):\n",
    "        # params: input_channels, out_channels, kernel_size,\n",
    "        # stride, padding, bias\n",
    "        return nn.ConvTranspose2d(params[0], params[1],\n",
    "               params[2], params[3], params[4], bias=params[5])\n",
    "    \n",
    "    @staticmethod\n",
    "    def BN2d(params):\n",
    "        # params: num_features\n",
    "        return nn.BatchNorm2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def IN2d(params):\n",
    "        #params: num_features\n",
    "        return nn.InstanceNorm2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Dropout(params):\n",
    "        #params: dropout ratio\n",
    "        return nn.Dropout(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(params):\n",
    "        # params: inplace\n",
    "        return nn.ReLU(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def LeakyReLU(params):\n",
    "        # params: negative_slope, inplace\n",
    "        return nn.LeakyReLU(params[0], params[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tanh(params):\n",
    "        return nn.Tanh()\n",
    "    \n",
    "    @staticmethod\n",
    "    def Sigmoid(params):\n",
    "        return nn.Sigmoid()\n",
    "    \n",
    "    @staticmethod\n",
    "    def AvgPool2d(params):\n",
    "        # params: kernel_size, stride, padding\n",
    "        return nn.AvgPool2d(params[0], params[1], params[2])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Interpolate(params):\n",
    "        # params: scale_factor, mode\n",
    "        return Interpolate_(params[0], params[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def RefPad2d(params):\n",
    "        return nn.ReflectionPad2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def RepPad2d(params):\n",
    "        return nn.ReplicationPad2d(params[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def Identity(params):\n",
    "        return Identity_()\n",
    "    \n",
    "    @staticmethod\n",
    "    def Squeeze(params):\n",
    "        return Squeeze_()\n",
    "    \n",
    "    @staticmethod\n",
    "    def ResBlock2d(params):\n",
    "        # params: fin, fout, kernel_size, padding_type,\n",
    "        # norm_type, use_dropout, bias, addon_ratio\n",
    "        return ResnetBlock(params[0], params[1], params[2], params[3],\n",
    "                           params[4], params[5], params[6], params[7])\n",
    "    \n",
    "class Block_mapping(object):\n",
    "    \n",
    "    module_mapping = {\n",
    "        \"Conv2d\":       Net_block.Conv2d,\n",
    "        \"ConvT2d\":      Net_block.ConvT2d,\n",
    "        \"BN2d\":         Net_block.BN2d,\n",
    "        \"IN2d\":         Net_block.IN2d,\n",
    "        \"Dropout\":      Net_block.Dropout,\n",
    "        \"Relu\":         Net_block.ReLU,\n",
    "        \"LeakyRelu\":    Net_block.LeakyReLU,\n",
    "        \"Tanh\":         Net_block.Tanh,\n",
    "        \"Sigmoid\":      Net_block.Sigmoid,\n",
    "        \"AvgPool2d\":    Net_block.AvgPool2d,\n",
    "        \"Interpolate\":  Net_block.Interpolate,\n",
    "        \"RefPad2d\":     Net_block.RefPad2d,\n",
    "        \"RepPad2d\":     Net_block.RepPad2d,\n",
    "        \"Squeeze\":      Net_block.Squeeze,\n",
    "        \"ResBlock2d\":   Net_block.ResBlock2d,\n",
    "        \"None\":         Net_block.Identity\n",
    "    }\n",
    "\n",
    "class Squeeze_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "class Identity_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class Interpolate_(nn.Module):\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super(Interpolate_, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "\n",
    "# ResnetBlock changese only C, NOT H or W\n",
    "class ResnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, fin, fout, kernel_size, padding_type, norm_type, use_dropout, use_bias, addon_ratio):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.addon_ratio = addon_ratio\n",
    "        self.conv_block, self.x_block = self.build_conv_block(fin, fout, kernel_size, padding_type, norm_type, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, fin, fout, kernel_size, padding_type, norm_type, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "            fin(int)            -- the number of channels in the input\n",
    "            fout(int)           -- the number of channels in the output\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        x_block    = []\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "            x_block    += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "            x_block    += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "            \n",
    "        if fin != fout:\n",
    "            x_block += [Block_mapping.module_mapping[\"Conv2d\"]([fin, fout, kernel_size, 1, p, False]),\n",
    "                        Block_mapping.module_mapping[norm_type]([fout])]\n",
    "        else:\n",
    "            x_block += [Block_mapping.module_mapping[\"None\"]([])]\n",
    "\n",
    "        conv_block += [\n",
    "            Block_mapping.module_mapping[\"Conv2d\"]([fin, fout, kernel_size, 1, p, use_bias]),\n",
    "            Block_mapping.module_mapping[norm_type]([fout]),\n",
    "            Block_mapping.module_mapping[\"Relu\"]([True])]\n",
    "        if use_dropout:\n",
    "            conv_block += [Block_mapping.module_mapping[\"Dropout\"]([0.5])]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [\n",
    "            Block_mapping.module_mapping[\"Conv2d\"]([fout, fout, kernel_size, 1, p, use_bias]),\n",
    "            Block_mapping.module_mapping[norm_type]([fout])]\n",
    "\n",
    "        return nn.Sequential(*conv_block), nn.Sequential(*x_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.x_block(x) + self.conv_block(x) * self.addon_ratio\n",
    "        return out "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptgpu] *",
   "language": "python",
   "name": "conda-env-ptgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
